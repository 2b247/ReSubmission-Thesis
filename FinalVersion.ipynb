{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ“˜ Thesis Project - Dropout Prediction\n",
    "# Section: 1. Data Loading & Preparation\n",
    "# Author: Tobias Benavides\n",
    "# Date: March 21, 2025\n",
    "# =====================================\n",
    "\n",
    "# ðŸ”§ Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "\n",
    "print(\"ðŸ”¥ Everything is working in the right environment!\")\n",
    "\n",
    "# ðŸ§¾ Project Context\n",
    "\"\"\"\n",
    "This notebook is part of the thesis project: \n",
    "\"Where did the students go? Leveraging Oversampling and Neural Networks to Predict University Dropouts\".\n",
    "\n",
    "Objective:\n",
    "- Load and explore the dataset from the UCI Machine Learning Repository.\n",
    "- Prepare the data for further preprocessing and modeling stages.\n",
    "- Ensure modularity and flexibility in case the dataset is updated or replaced.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ“‚ Dataset Path (change this if needed)\n",
    "DATA_PATH = \"data/data.csv\"\n",
    "\n",
    "# âœ… Load Dataset\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, delimiter=';')\n",
    "    print(f\"âœ… Data successfully loaded! Shape: {df.shape}\")\n",
    "    df_cleaned = df.copy()  # Save a clean working copy\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ File not found at path: {DATA_PATH}\")\n",
    "    df = None\n",
    "\n",
    "# âœ… Initial Cleaning & Checks\n",
    "if df is not None:\n",
    "    df.columns = df.columns.str.strip()  # clean column names\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    display(df.describe(include='all'))\n",
    "\n",
    "    # Check missing values and duplicates\n",
    "    print(\"ðŸ” Missing values per column:\\n\", df.isnull().sum())\n",
    "    print(f\"ðŸ” Duplicate rows found: {df.duplicated().sum()}\")\n",
    "\n",
    "    # Remove 'Enrolled' class and encode target\n",
    "    df = df[df[\"Target\"] != \"Enrolled\"]\n",
    "    df[\"Target_Encoded\"] = df[\"Target\"].map({\"Dropout\": 0, \"Graduate\": 1})\n",
    "    df.drop(columns=[\"Target\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.drop(columns=[\"Target_Encoded\"])\n",
    "    y = df[\"Target_Encoded\"]\n",
    "\n",
    "    # Visual sanity check\n",
    "    sns.countplot(x=y)\n",
    "    plt.title(\"Distribution of Dropout vs Graduate\")\n",
    "    plt.xlabel(\"Target Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks([0, 1], ['Dropout', 'Graduate'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) First EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ“Š Section: Exploratory Data Analysis (EDA)\n",
    "# Including: Target Encoding + Feature Grouping\n",
    "# =====================================\n",
    "\n",
    "# ðŸŽ¯ Confirm target column and encode\n",
    "TARGET_COL = 'Target'\n",
    "label_mapping = {'Dropout': 0, 'Graduate': 1, 'Enrolled': 2}  # Enrolled will be removed later\n",
    "df_cleaned['Target_Encoded'] = df_cleaned[TARGET_COL].map(label_mapping)\n",
    "\n",
    "# ðŸŸ¢ Class Distribution Plot (original labels)\n",
    "target_counts = df_cleaned[TARGET_COL].value_counts(normalize=True).sort_index()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=target_counts.index, y=target_counts.values)\n",
    "plt.title(\"ðŸŽ¯ Target Class Distribution\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Class Distribution:\")\n",
    "display(df_cleaned[TARGET_COL].value_counts())\n",
    "\n",
    "# ðŸ—‚ï¸ Feature Groups (based on thesis proposal)\n",
    "demographic_cols = ['Gender', 'Age at enrollment', 'Nationality', 'Marital status']\n",
    "socioeconomic_cols = [\"Scholarship holder\", \"Father's occupation\", \"Mother's qualification\"]\n",
    "academic_cols = [\n",
    "    'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (approved)', \n",
    "    'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (approved)'\n",
    "]\n",
    "macroeconomic_cols = ['Unemployment rate', 'Inflation rate', 'GDP']\n",
    "\n",
    "# ðŸŽ¨ Plotting Helper Function\n",
    "def plot_feature_distributions(feature_list, title_prefix):\n",
    "    for col in feature_list:\n",
    "        if col in df_cleaned.columns:\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            if df_cleaned[col].dtype in ['object', 'category']:\n",
    "                sns.countplot(data=df_cleaned, x=col, hue=TARGET_COL)\n",
    "            else:\n",
    "                sns.boxplot(data=df_cleaned, x=TARGET_COL, y=col)\n",
    "            plt.title(f\"{title_prefix}: {col}\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# ðŸ§¬ Demographic Features\n",
    "plot_feature_distributions(demographic_cols, \"ðŸ§¬ Demographic\")\n",
    "\n",
    "# ðŸ’° Socio-Economic Features\n",
    "plot_feature_distributions(socioeconomic_cols, \"ðŸ’° Socio-Economic\")\n",
    "\n",
    "# ðŸŽ“ Academic Features\n",
    "plot_feature_distributions(academic_cols, \"ðŸŽ“ Academic\")\n",
    "\n",
    "# ðŸŒ Macroeconomic Features\n",
    "plot_feature_distributions(macroeconomic_cols, \"ðŸŒ Macroeconomic\")\n",
    "\n",
    "# ðŸ” Correlation Matrix (numeric only, including encoded target)\n",
    "numerical_df = df_cleaned.select_dtypes(include='number')\n",
    "\n",
    "if numerical_df.shape[1] == 0:\n",
    "    print(\"âš ï¸ No numerical columns found â€” skipping correlation matrix.\")\n",
    "else:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation = numerical_df.corr()\n",
    "    sns.heatmap(correlation, annot=False, cmap='coolwarm', fmt=\".2f\", center=0)\n",
    "    plt.title(\"ðŸ” Correlation Matrix of Numerical Features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Further feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ” Enhanced EDA: Advanced Feature Insights\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import kruskal, chi2_contingency, shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# âœ… Setup\n",
    "df_eda = df_cleaned.copy()  # df_cleaned already has 'Target_Encoded'\n",
    "\n",
    "# ðŸŽ¯ Drop 'Target' for clarity if it exists\n",
    "if \"Target\" in df_eda.columns:\n",
    "    df_eda.drop(columns=[\"Target\"], inplace=True)\n",
    "\n",
    "# 1. ðŸ”Ž Missing Value Matrix\n",
    "msno.matrix(df_eda)\n",
    "plt.title(\"Missing Values Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 2. ðŸ§  Multicollinearity - Variance Inflation Factor (VIF)\n",
    "numeric_features = df_eda.select_dtypes(include=np.number).drop(columns=['Target_Encoded'])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(numeric_features)\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': numeric_features.columns,\n",
    "    'VIF': [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "}).sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=vif_data, x='VIF', y='Feature')\n",
    "plt.title(\"Variance Inflation Factor (VIF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. ðŸ“Š Feature Importance via Random Forest\n",
    "X = df_eda.drop(columns=['Target_Encoded'])\n",
    "y = df_eda['Target_Encoded']\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importances.head(15), x='Importance', y='Feature')\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. ðŸ“ˆ Kruskal-Wallis Test (for numeric vs Target_Encoded)\n",
    "print(\"ðŸ”¬ Kruskal-Wallis Test (Numerical features vs Target):\")\n",
    "for col in numeric_features.columns:\n",
    "    groups = [df_eda[df_eda[\"Target_Encoded\"] == val][col] for val in df_eda[\"Target_Encoded\"].unique()]\n",
    "    stat, p = kruskal(*groups)\n",
    "    print(f\" - {col}: H = {stat:.2f}, p = {p:.4f}\")\n",
    "\n",
    "# 5. ðŸ§ª Chi-Square Test (for categorical vs Target_Encoded)\n",
    "print(\"\\nðŸ”¬ Chi-Square Test (Categorical features vs Target):\")\n",
    "categorical_cols = df_eda.select_dtypes(include='int64').columns.difference(numeric_features.columns)\n",
    "for col in categorical_cols:\n",
    "    contingency = pd.crosstab(df_eda[col], df_eda[\"Target_Encoded\"])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency)\n",
    "    print(f\" - {col}: Chi2 = {chi2:.2f}, p = {p:.4f}\")\n",
    "\n",
    "# 6. ðŸ§¬ Normality & Skewness Check\n",
    "print(\"\\nðŸ“ Skewness & Normality Test (Shapiro-Wilk):\")\n",
    "for col in numeric_features.columns:\n",
    "    skew_val = df_eda[col].skew()\n",
    "    _, p_shapiro = shapiro(df_eda[col])\n",
    "    print(f\" - {col}: skew = {skew_val:.2f}, Shapiro p = {p_shapiro:.4f}\")\n",
    "\n",
    "# 7. ðŸŽ¯ PCA Visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaler.fit_transform(numeric_features))\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = df_eda['Target_Encoded']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target', palette='coolwarm')\n",
    "plt.title(\"PCA Projection of Numerical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. ðŸ§Š Outlier Detection via Boxplots\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=numeric_features)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Boxplots for Outlier Detection (Numeric Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. ðŸ”— Pairplot (Feature Relationships + Target)\n",
    "selected_pairplot_cols = numeric_features.columns[:4].tolist() + ['Target_Encoded']\n",
    "sns.pairplot(df_eda[selected_pairplot_cols], hue='Target_Encoded')\n",
    "plt.suptitle(\"Pairwise Feature Relationships\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dropping the \"Enrolled\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ› ï¸ Section: Feature Engineering & Preprocessing (Improved)\n",
    "# =====================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. âœ… Remove \"Enrolled\" class from dataset (for final modeling)\n",
    "df_model = df_cleaned[df_cleaned['Target'] != 'Enrolled'].copy()\n",
    "print(f\"âœ… Shape after removing 'Enrolled': {df_model.shape}\")\n",
    "\n",
    "# 2. ðŸŽ¯ Encode target variable: Dropout = 0, Graduate = 1\n",
    "target_map = {'Dropout': 0, 'Graduate': 1}\n",
    "df_model['Target_Encoded'] = df_model['Target'].map(target_map)\n",
    "\n",
    "# (Optional) Remove original target column now to avoid drop errors later\n",
    "df_model.drop(columns='Target', inplace=True)\n",
    "\n",
    "# 3. ðŸ§¼ Encode categorical variables using LabelEncoder\n",
    "# Safe column selection with error-tolerant drop\n",
    "categorical_cols = df_model.select_dtypes(include=['object']).columns\n",
    "le_dict = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    le_dict[col] = le  # Save encoders for inverse transform later\n",
    "\n",
    "print(f\"âœ… Categorical encoding complete. Encoded columns: {list(categorical_cols)}\")\n",
    "\n",
    "# 4. ðŸ§ª Split features/target\n",
    "X = df_model.drop(columns='Target_Encoded')\n",
    "y = df_model['Target_Encoded']\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"ðŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"ðŸŽ¯ Target vector shape: {y.shape}\")\n",
    "print(\"ðŸŽ¯ Target class distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 New Class Distribution Graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ðŸŸ¦ Plot target class distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y, palette='pastel')\n",
    "plt.title(\"ðŸŽ¯ Final Target Class Distribution\")\n",
    "plt.xlabel(\"Target Class (0 = Dropout, 1 = Graduate)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.xticks(ticks=[0, 1], labels=['Dropout', 'Graduate'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Enhanced EDA (Post-dropping \"Enrolled\" class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ” Enhanced EDA: Post-Preprocessing Analysis\n",
    "# =====================================\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import kruskal, chi2_contingency, shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. ðŸ§¼ Missing Value Matrix\n",
    "msno.matrix(df_model)\n",
    "plt.title(\"Missing Values Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 2. ðŸ§  Variance Inflation Factor (VIF)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'VIF': [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "}).sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=vif_data, x='VIF', y='Feature')\n",
    "plt.title(\"Multicollinearity Check: Variance Inflation Factor (VIF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. ðŸŒ² Feature Importance via Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importances.head(15), x='Importance', y='Feature')\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. ðŸ§ª Kruskal-Wallis Test (Numerical vs Target)\n",
    "print(\"ðŸ”¬ Kruskal-Wallis Test (Numerical features vs Target):\")\n",
    "numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "for col in numeric_cols:\n",
    "    groups = [X[y == val][col] for val in sorted(y.unique())]\n",
    "    stat, p = kruskal(*groups)\n",
    "    print(f\" - {col}: H = {stat:.2f}, p = {p:.4f}\")\n",
    "\n",
    "# 5. ðŸ§ª Chi-Square Test (Categorical vs Target)\n",
    "print(\"\\nðŸ”¬ Chi-Square Test (Categorical features vs Target):\")\n",
    "categorical_cols = X.select_dtypes(include='int64').columns.difference(numeric_cols)\n",
    "for col in categorical_cols:\n",
    "    contingency = pd.crosstab(X[col], y)\n",
    "    chi2, p, _, _ = chi2_contingency(contingency)\n",
    "    print(f\" - {col}: Chi2 = {chi2:.2f}, p = {p:.4f}\")\n",
    "\n",
    "# 6. ðŸ§ª Skewness & Normality Test (Shapiro-Wilk)\n",
    "print(\"\\nðŸ“ Skewness & Normality:\")\n",
    "for col in numeric_cols:\n",
    "    skew_val = X[col].skew()\n",
    "    _, p_shapiro = shapiro(X[col])\n",
    "    print(f\" - {col}: skew = {skew_val:.2f}, Shapiro p = {p_shapiro:.4f}\")\n",
    "\n",
    "# 7. ðŸŒ PCA Projection\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y.values\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target', palette='coolwarm')\n",
    "plt.title(\"PCA Projection of Feature Space\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. ðŸ§Š Outlier Detection via Boxplots\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=X[numeric_cols])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Outlier Detection (Boxplots of Numeric Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. ðŸ”— Pairplot of Top 4 Numeric Features\n",
    "top4 = importances.head(4)['Feature'].tolist()\n",
    "sns.pairplot(df_model[top4 + ['Target_Encoded']], hue='Target_Encoded')\n",
    "plt.suptitle(\"Pairwise Relationships of Top Features\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Actual Feature engeeniring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ðŸ§  Final Feature Engineering + PCA + SHAP (Safe Version)\n",
    "# =============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Filter and encode target\n",
    "df_model = df_cleaned[df_cleaned['Target'] != 'Enrolled'].copy()\n",
    "df_model['Target_Encoded'] = df_model['Target'].map({'Dropout': 0, 'Graduate': 1})\n",
    "df_model.drop(columns='Target', inplace=True)\n",
    "\n",
    "# Step 2: Drop irrelevant/redundant features\n",
    "drop_cols = [\n",
    "    \"Course\", \"Nacionality\", \"International\", \"Educational special needs\",\n",
    "    \"Curricular units 1st sem (credited)\", \"Curricular units 2nd sem (credited)\",\n",
    "    \"Mother's occupation\", \"Father's occupation\", \"Application order\",\n",
    "    \"Mother's qualification\", \"Father's qualification\"\n",
    "]\n",
    "df_model.drop(columns=[col for col in drop_cols if col in df_model.columns], inplace=True)\n",
    "\n",
    "# Step 3: Label encode categorical features\n",
    "le_dict = {}\n",
    "categorical_cols = df_model.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Step 4: Composite academic performance features\n",
    "df_model['performance_1st'] = df_model['Curricular units 1st sem (approved)'] + df_model['Curricular units 1st sem (grade)']\n",
    "df_model['performance_2nd'] = df_model['Curricular units 2nd sem (approved)'] + df_model['Curricular units 2nd sem (grade)']\n",
    "df_model.drop(columns=[\n",
    "    'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)'\n",
    "], inplace=True)\n",
    "\n",
    "# Step 5: Bin skewed variables\n",
    "df_model['age_bin'] = pd.cut(df_model['Age at enrollment'], bins=[15, 20, 25, 30, 50], labels=[0, 1, 2, 3])\n",
    "df_model['admission_bin'] = pd.cut(df_model['Admission grade'], bins=[0, 10, 13, 20], labels=[0, 1, 2])\n",
    "df_model.drop(columns=['Age at enrollment', 'Admission grade'], inplace=True)\n",
    "\n",
    "# Step 6: Select final feature set\n",
    "selected_features = [\n",
    "    'performance_1st', 'performance_2nd',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 2nd sem (evaluations)',\n",
    "    'Tuition fees up to date', 'Debtor', 'Scholarship holder',\n",
    "    'Gender', 'age_bin', 'admission_bin'\n",
    "]\n",
    "X = df_model[selected_features].copy()\n",
    "y = df_model['Target_Encoded']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"âœ… Final shape of X: {X.shape}\")\n",
    "print(f\"âœ… Final shape of y: {y.shape}\")\n",
    "print(f\"ðŸ“‹ Features used: {feature_names}\")\n",
    "\n",
    "# Step 7: PCA for 2D visualization\n",
    "X_encoded = pd.get_dummies(X, drop_first=True).astype(float)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "pca_df = pd.DataFrame(pca_components, columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y.values\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target', palette='coolwarm')\n",
    "plt.title(\"ðŸŽ¯ PCA Projection of Final Feature Space\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 8: SHAP Explanations (Robust fix for binary classification)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_encoded, y)\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_encoded)\n",
    "\n",
    "# Handle SHAP values structure (list for binary classification)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]  # class = Graduate\n",
    "else:\n",
    "    shap_vals = shap_values\n",
    "\n",
    "# Check shapes before plotting\n",
    "print(\"ðŸ” SHAP matrix shape:\", np.array(shap_vals).shape)\n",
    "print(\"ðŸ“Š Feature matrix shape:\", X_encoded.shape)\n",
    "\n",
    "# SHAP summary plot\n",
    "shap.summary_plot(shap_vals, features=X_encoded, feature_names=X_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Baseline model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Random Forrest - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ðŸŒ² Random Forest Baseline + Full Evaluation\n",
    "# Using Final Feature Selection Set\n",
    "# ============================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    mean_squared_error, log_loss\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. âœ‚ï¸ Train-test split (80/20) â€” stratify to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. ðŸŒ² Initialize Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. ðŸ§  Train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 4. ðŸ” Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 5. ðŸ“Š Evaluation Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "mse = mean_squared_error(y_test, y_proba)\n",
    "logloss = log_loss(y_test, y_proba)\n",
    "\n",
    "print(\"ðŸ“Š Evaluation on Test Set:\")\n",
    "print(f\"âœ… Accuracy:       {acc:.4f}\")\n",
    "print(f\"âœ… Precision:      {prec:.4f}\")\n",
    "print(f\"âœ… Recall:         {rec:.4f}\")\n",
    "print(f\"âœ… F1 Score:       {f1:.4f}\")\n",
    "print(f\"âœ… ROC-AUC:        {roc_auc:.4f}\")\n",
    "print(f\"âœ… MSE:            {mse:.4f}\")\n",
    "print(f\"âœ… Log Loss:       {logloss:.4f}\")\n",
    "\n",
    "# 6. ðŸ“œ Classification Report\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Dropout\", \"Graduate\"]))\n",
    "\n",
    "# 7. ðŸ” Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    rf, X_test, y_test, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: Random Forest (Final Feature Set)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 8. ðŸ“ˆ ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"Random Forest\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: Random Forest (Final Feature Set)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 9. ðŸ” Cross-validated scores (5-fold)\n",
    "cv_f1 = cross_val_score(rf, X_encoded, y, scoring='f1', cv=5)\n",
    "cv_auc = cross_val_score(rf, X_encoded, y, scoring='roc_auc', cv=5)\n",
    "\n",
    "print(f\"\\nðŸ” Cross-validated F1 Scores:       {cv_f1}\")\n",
    "print(f\"ðŸ“Œ Mean F1 Score (5-fold):          {cv_f1.mean():.4f}\")\n",
    "print(f\"\\nðŸ” Cross-validated ROC-AUC Scores:  {cv_auc}\")\n",
    "print(f\"ðŸ“Œ Mean ROC-AUC Score (5-fold):     {cv_auc.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) CatBoost - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ðŸ± CatBoost Baseline Model â€” Final Feature Set\n",
    "# Full Metrics + CV + ROC + Confusion Matrix\n",
    "# =============================================\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    mean_squared_error, log_loss\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. âœ‚ï¸ Train-test split (80/20) using final encoded features\n",
    "X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. ðŸ± Initialize CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    verbose=0,              # Suppress training output\n",
    "    random_seed=42,\n",
    "    eval_metric='F1',\n",
    "    class_weights=[1, 1]    # Equal weights for baseline\n",
    ")\n",
    "\n",
    "# 3. ðŸ§ª Train the model\n",
    "catboost_model.fit(X_train_cb, y_train_cb)\n",
    "\n",
    "# 4. ðŸ” Predict on test set\n",
    "y_pred_cb = catboost_model.predict(X_test_cb)\n",
    "y_proba_cb = catboost_model.predict_proba(X_test_cb)[:, 1]\n",
    "\n",
    "# 5. ðŸ“Š Evaluation Metrics\n",
    "acc_cb = accuracy_score(y_test_cb, y_pred_cb)\n",
    "prec_cb = precision_score(y_test_cb, y_pred_cb)\n",
    "rec_cb = recall_score(y_test_cb, y_pred_cb)\n",
    "f1_cb = f1_score(y_test_cb, y_pred_cb)\n",
    "roc_auc_cb = roc_auc_score(y_test_cb, y_proba_cb)\n",
    "mse_cb = mean_squared_error(y_test_cb, y_proba_cb)\n",
    "logloss_cb = log_loss(y_test_cb, y_proba_cb)\n",
    "\n",
    "print(\"ðŸ“Š Evaluation on Test Set (CatBoost):\")\n",
    "print(f\"âœ… Accuracy:       {acc_cb:.4f}\")\n",
    "print(f\"âœ… Precision:      {prec_cb:.4f}\")\n",
    "print(f\"âœ… Recall:         {rec_cb:.4f}\")\n",
    "print(f\"âœ… F1 Score:       {f1_cb:.4f}\")\n",
    "print(f\"âœ… ROC-AUC:        {roc_auc_cb:.4f}\")\n",
    "print(f\"âœ… MSE:            {mse_cb:.4f}\")\n",
    "print(f\"âœ… Log Loss:       {logloss_cb:.4f}\")\n",
    "\n",
    "# 6. ðŸ“‹ Classification Report\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test_cb, y_pred_cb, target_names=[\"Dropout\", \"Graduate\"]))\n",
    "\n",
    "# 7. ðŸ” Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test_cb, y_pred_cb, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: CatBoost (Final Feature Set)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 8. ðŸ“ˆ ROC Curve\n",
    "fpr_cb, tpr_cb, _ = roc_curve(y_test_cb, y_proba_cb)\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr_cb, tpr=tpr_cb, roc_auc=roc_auc_cb, estimator_name=\"CatBoost\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: CatBoost (Final Feature Set)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 9. ðŸ” Cross-validated metrics (5-fold)\n",
    "cv_f1_cb = cross_val_score(catboost_model, X_encoded, y, scoring='f1', cv=5)\n",
    "cv_auc_cb = cross_val_score(catboost_model, X_encoded, y, scoring='roc_auc', cv=5)\n",
    "\n",
    "print(f\"\\nðŸ” Cross-validated F1 Scores:       {cv_f1_cb}\")\n",
    "print(f\"ðŸ“Œ Mean F1 Score (5-fold):          {cv_f1_cb.mean():.4f}\")\n",
    "print(f\"\\nðŸ” Cross-validated ROC-AUC Scores:  {cv_auc_cb}\")\n",
    "print(f\"ðŸ“Œ Mean ROC-AUC Score (5-fold):     {cv_auc_cb.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) ANN - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, roc_auc_score, roc_curve, auc,\n",
    "    precision_score, recall_score, log_loss, mean_squared_error, confusion_matrix\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# === Setup ===\n",
    "f1_scores, acc_scores, auc_scores = [], [], []\n",
    "prec_scores, rec_scores, logloss_scores, mse_scores = [], [], [], []\n",
    "conf_matrices = []\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "X_array = X_encoded.to_numpy()\n",
    "y_array = y.to_numpy()\n",
    "\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# === Cross-Validation ===\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_array, y_array), 1):\n",
    "    print(f\"\\nðŸ” Fold {fold}/{n_splits}\")\n",
    "    X_train, X_test = X_array[train_idx], X_array[test_idx]\n",
    "    y_train, y_test = y_array[train_idx], y_array[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=32,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "              verbose=0)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_scaled).ravel()\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    prec_scores.append(precision_score(y_test, y_pred))\n",
    "    rec_scores.append(recall_score(y_test, y_pred))\n",
    "    auc_val = roc_auc_score(y_test, y_pred_probs)\n",
    "    auc_scores.append(auc_val)\n",
    "    logloss_scores.append(log_loss(y_test, y_pred_probs))\n",
    "    mse_scores.append(mean_squared_error(y_test, y_pred_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    aucs.append(auc_val)\n",
    "    plt.plot(fpr, tpr, alpha=0.3, label=f'Fold {fold} ROC (AUC = {auc_val:.2f})')\n",
    "\n",
    "# === ROC Curve ===\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f} Â± {std_auc:.2f})', lw=2)\n",
    "plt.title('ðŸ“ˆ ROC Curve - ANN (5-Fold CV)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Averaged Confusion Matrix ===\n",
    "avg_cm = np.mean(conf_matrices, axis=0).astype(int)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Dropout', 'Graduate'],\n",
    "            yticklabels=['Dropout', 'Graduate'])\n",
    "plt.title(\"ðŸ” Averaged Confusion Matrix (ANN - 5-Fold CV)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === SHAP KernelExplainer (fully CPU compatible) ===\n",
    "X_train_sample = X_train_scaled[:100]\n",
    "X_test_sample = X_test_scaled[:100]\n",
    "\n",
    "predict_fn = lambda x: model.predict(x).ravel()\n",
    "explainer = shap.KernelExplainer(predict_fn, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "shap.summary_plot(shap_values, features=X_test_sample, feature_names=X_encoded.columns)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\nðŸ“Š ANN Baseline Summary:\")\n",
    "print(f\"âœ… Accuracy:   {np.mean(acc_scores):.4f} Â± {np.std(acc_scores):.4f}\")\n",
    "print(f\"âœ… Precision:  {np.mean(prec_scores):.4f} Â± {np.std(prec_scores):.4f}\")\n",
    "print(f\"âœ… Recall:     {np.mean(rec_scores):.4f} Â± {np.std(rec_scores):.4f}\")\n",
    "print(f\"âœ… F1 Score:   {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {np.mean(auc_scores):.4f} Â± {np.std(auc_scores):.4f}\")\n",
    "print(f\"âœ… MSE:        {np.mean(mse_scores):.4f} Â± {np.std(mse_scores):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {np.mean(logloss_scores):.4f} Â± {np.std(logloss_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SMOTE Oversampling Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Random Forrest + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ðŸŒ² Random Forest with SMOTE â€” Enhanced Metrics + CV (Fixed)\n",
    "# =========================================\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    mean_squared_error, log_loss\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "X_imputed = imputer.fit_transform(X)  # For CV later\n",
    "\n",
    "# 3. Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 4. Train the model\n",
    "rf_smote = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = rf_smote.predict(X_test)\n",
    "y_proba = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 6. Metrics\n",
    "print(\"ðŸ“Š Classification Report (Random Forest + SMOTE):\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Dropout\", \"Graduate\"]))\n",
    "\n",
    "print(\"ðŸ“Œ Performance on Test Set:\")\n",
    "print(f\"âœ… Accuracy:   {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Precision:  {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Recall:     {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… F1 Score:   {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… MSE:        {mean_squared_error(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {log_loss(y_test, y_proba):.4f}\")\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    rf_smote, X_test, y_test, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: RF + SMOTE\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 8. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"RF + SMOTE\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: Random Forest + SMOTE\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Cross-Validation (with imputed X)\n",
    "f1_cv = cross_val_score(rf_smote, X_imputed, y, scoring='f1', cv=5)\n",
    "roc_auc_cv = cross_val_score(rf_smote, X_imputed, y, scoring='roc_auc', cv=5)\n",
    "\n",
    "print(\"ðŸ” Cross-Validation Results (5-Fold):\")\n",
    "print(f\"ðŸ“Œ Mean F1 Score:   {f1_cv.mean():.4f} Â± {f1_cv.std():.4f}\")\n",
    "print(f\"ðŸ“Œ Mean ROC-AUC:    {roc_auc_cv.mean():.4f} Â± {roc_auc_cv.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 CatBoost + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ðŸ± CatBoost with SMOTE â€” Full Metrics\n",
    "# =========================================\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    log_loss, mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# 3. SMOTE oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# 4. Train CatBoost model\n",
    "model_cb_smote = CatBoostClassifier(verbose=0, random_seed=42, eval_metric='F1')\n",
    "model_cb_smote.fit(X_smote, y_smote)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = model_cb_smote.predict(X_test_imputed)\n",
    "y_proba = model_cb_smote.predict_proba(X_test_imputed)[:, 1]\n",
    "\n",
    "# 6. Evaluation Metrics\n",
    "print(\"ðŸ“Š Classification Report: CatBoost (SMOTE)\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nðŸ“Œ Test Metrics:\")\n",
    "print(f\"âœ… F1 Score:   {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Accuracy:   {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Precision:  {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Recall:     {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… MSE:        {mean_squared_error(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {log_loss(y_test, y_proba):.4f}\")\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: CatBoost (SMOTE)\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"CatBoost (SMOTE)\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: CatBoost (SMOTE)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ANN + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ¤– ANN + SMOTE (5-Fold CV) â€” Final Version with Imputation + SHAP\n",
    "# =====================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, roc_auc_score, roc_curve, auc,\n",
    "    precision_score, recall_score, log_loss, mean_squared_error, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup\n",
    "f1_scores, acc_scores, auc_scores = [], [], []\n",
    "prec_scores, rec_scores, logloss_scores, mse_scores = [], [], [], []\n",
    "conf_matrices = []\n",
    "\n",
    "n_splits = 5\n",
    "random_seed = 42\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "\n",
    "X_array = X.to_numpy()\n",
    "y_array = y.to_numpy()\n",
    "\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_array, y_array), 1):\n",
    "    print(f\"\\nðŸ” Fold {fold}/{n_splits}\")\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test = X_array[train_idx], X_array[test_idx]\n",
    "    y_train, y_test = y_array[train_idx], y_array[test_idx]\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Impute missing values (mean strategy)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_scaled = imputer.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = imputer.transform(X_test_scaled)\n",
    "\n",
    "    # SMOTE oversampling\n",
    "    smote = SMOTE(random_state=random_seed)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    # ANN model\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train_resampled, y_train_resampled,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_probs = model.predict(X_test_scaled).ravel()\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    prec_scores.append(precision_score(y_test, y_pred))\n",
    "    rec_scores.append(recall_score(y_test, y_pred))\n",
    "    auc_val = roc_auc_score(y_test, y_pred_probs)\n",
    "    auc_scores.append(auc_val)\n",
    "    logloss_scores.append(log_loss(y_test, y_pred_probs))\n",
    "    mse_scores.append(mean_squared_error(y_test, y_pred_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    aucs.append(auc_val)\n",
    "    plt.plot(fpr, tpr, alpha=0.3, label=f'Fold {fold} ROC (AUC = {auc_val:.2f})')\n",
    "\n",
    "# ðŸŽ¯ Final ROC Curve\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f} Â± {std_auc:.2f})', lw=2)\n",
    "plt.title('ðŸ“ˆ ROC Curve - ANN + SMOTE (5-Fold CV)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ” Averaged Confusion Matrix\n",
    "avg_cm = np.mean(conf_matrices, axis=0).astype(int)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Dropout', 'Graduate'],\n",
    "            yticklabels=['Dropout', 'Graduate'])\n",
    "plt.title(\"ðŸ” Averaged Confusion Matrix (ANN + SMOTE)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ” SHAP with KernelExplainer (CPU-friendly)\n",
    "X_sample_train = X_train_resampled[:100]\n",
    "X_sample_test = X_test_scaled[:100]\n",
    "predict_fn = lambda x: model.predict(x).ravel()\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_fn, X_sample_train)\n",
    "shap_values = explainer.shap_values(X_sample_test)\n",
    "shap.summary_plot(shap_values, features=X_sample_test, feature_names=X.columns)\n",
    "\n",
    "# ðŸ“Š Final Metrics Summary\n",
    "print(\"\\nðŸ“Š ANN + SMOTE Summary:\")\n",
    "print(f\"âœ… Accuracy:   {np.mean(acc_scores):.4f} Â± {np.std(acc_scores):.4f}\")\n",
    "print(f\"âœ… Precision:  {np.mean(prec_scores):.4f} Â± {np.std(prec_scores):.4f}\")\n",
    "print(f\"âœ… Recall:     {np.mean(rec_scores):.4f} Â± {np.std(rec_scores):.4f}\")\n",
    "print(f\"âœ… F1 Score:   {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {np.mean(auc_scores):.4f} Â± {np.std(auc_scores):.4f}\")\n",
    "print(f\"âœ… MSE:        {np.mean(mse_scores):.4f} Â± {np.std(mse_scores):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {np.mean(logloss_scores):.4f} Â± {np.std(logloss_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ADASYN Oversampling Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Random Forrest + ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ðŸŒ² Random Forest with ADASYN â€” Full Metrics (Fixed)\n",
    "# =========================================\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    log_loss, mean_squared_error\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Impute missing values (required for ADASYN)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# 3. ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# 4. Train model\n",
    "rf_adasyn = RandomForestClassifier(random_state=42)\n",
    "rf_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = rf_adasyn.predict(X_test_imputed)\n",
    "y_proba = rf_adasyn.predict_proba(X_test_imputed)[:, 1]\n",
    "\n",
    "# 6. Evaluation Metrics\n",
    "print(\"ðŸ“Š Classification Report: Random Forest (ADASYN)\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nðŸ“Œ Test Metrics:\")\n",
    "print(f\"âœ… F1 Score:   {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Accuracy:   {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Precision:  {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Recall:     {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… MSE:        {mean_squared_error(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {log_loss(y_test, y_proba):.4f}\")\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: Random Forest (ADASYN)\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"RF (ADASYN)\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: Random Forest (ADASYN)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 CatBoost + ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ðŸ± CatBoost with ADASYN â€” Full Metrics + Extended Error Analysis\n",
    "# =========================================\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    log_loss, mean_squared_error\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# 2.1 Handle possible column mismatch\n",
    "X_columns = X.columns\n",
    "if X_test_imputed.shape[1] != len(X_columns):\n",
    "    X_columns = X_columns[:X_test_imputed.shape[1]]\n",
    "X_test_df = pd.DataFrame(X_test_imputed, columns=X_columns)\n",
    "\n",
    "# 3. ADASYN Oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# 4. Train CatBoost\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "cat_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = cat_model.predict(X_test_imputed)\n",
    "y_proba = cat_model.predict_proba(X_test_imputed)[:, 1]\n",
    "\n",
    "# 6. Evaluation Metrics\n",
    "print(\"ðŸ“Š Classification Report: CatBoost (ADASYN)\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nðŸ“Œ Test Metrics:\")\n",
    "print(f\"âœ… F1 Score:   {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Accuracy:   {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Precision:  {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… Recall:     {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… MSE:        {mean_squared_error(y_test, y_proba):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {log_loss(y_test, y_proba):.4f}\")\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, display_labels=[\"Dropout\", \"Graduate\"], cmap='Blues'\n",
    ")\n",
    "plt.title(\"ðŸ” Confusion Matrix: CatBoost (ADASYN)\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"CatBoost (ADASYN)\").plot()\n",
    "plt.title(\"ðŸ“ˆ ROC Curve: CatBoost (ADASYN)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. SHAP Analysis\n",
    "print(\"\\nðŸ”Ž SHAP Summary Plot (Class: Graduate):\")\n",
    "explainer = shap.TreeExplainer(cat_model)\n",
    "shap_values = explainer.shap_values(X_test_df)\n",
    "\n",
    "# Plot SHAP summary and store for reuse\n",
    "if isinstance(shap_values, list):\n",
    "    shap.summary_plot(shap_values[1], X_test_df, plot_type=\"bar\")\n",
    "    shap_array = shap_values[1]\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\")\n",
    "    shap_array = shap_values\n",
    "\n",
    "# Top feature for follow-up\n",
    "top_feature_index = np.argmax(np.abs(np.mean(shap_array, axis=0)))\n",
    "top_feature = X_columns[top_feature_index]\n",
    "\n",
    "# 10. SHAP Dependence Plot\n",
    "print(f\"\\nðŸ“ˆ SHAP Dependence Plot for: {top_feature}\")\n",
    "shap.dependence_plot(top_feature, shap_array, X_test_df)\n",
    "\n",
    "# 11. Align indices\n",
    "y_test_series = pd.Series(y_test, index=X_test_df.index)\n",
    "y_pred_series = pd.Series(y_pred, index=X_test_df.index)\n",
    "\n",
    "# 12. TP and FN Identification\n",
    "TP_idx = (y_test_series == 1) & (y_pred_series == 1)\n",
    "FN_idx = (y_test_series == 1) & (y_pred_series == 0)\n",
    "\n",
    "tp_values = X_test_df.loc[TP_idx, top_feature]\n",
    "fn_values = X_test_df.loc[FN_idx, top_feature]\n",
    "\n",
    "# 13. Cohenâ€™s d\n",
    "mean_diff = tp_values.mean() - fn_values.mean()\n",
    "pooled_std = np.sqrt((tp_values.std()**2 + fn_values.std()**2) / 2)\n",
    "cohen_d = mean_diff / pooled_std\n",
    "print(f\"\\nðŸ“ Cohenâ€™s d for '{top_feature}' between TP and FN: {cohen_d:.2f}\")\n",
    "\n",
    "# 14. T-test\n",
    "t_stat, p_val = ttest_ind(tp_values, fn_values, equal_var=False)\n",
    "print(f\"ðŸ“Š T-test p-value for '{top_feature}': {p_val:.4f}\")\n",
    "\n",
    "# 15. Plot distribution: TP vs FN for top feature\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(tp_values, label=\"True Positives\", fill=True)\n",
    "sns.kdeplot(fn_values, label=\"False Negatives\", fill=True)\n",
    "plt.title(f\"ðŸ” Distribution of '{top_feature}' for TP vs FN\")\n",
    "plt.xlabel(top_feature)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 16. Error Breakdown by Gender (or other categorical)\n",
    "if \"Gender\" in X_test_df.columns:\n",
    "    gender_df = X_test_df.copy()\n",
    "    gender_df[\"TrueLabel\"] = y_test_series\n",
    "    gender_df[\"Prediction\"] = y_pred_series\n",
    "    gender_df[\"Correct\"] = gender_df[\"TrueLabel\"] == gender_df[\"Prediction\"]\n",
    "    \n",
    "    gender_accuracy = gender_df.groupby(\"Gender\")[\"Correct\"].mean()\n",
    "    print(\"\\nðŸ“Š Accuracy by Gender:\")\n",
    "    print(gender_accuracy)\n",
    "\n",
    "    gender_counts = gender_df.groupby([\"Gender\", \"Correct\"]).size().unstack()\n",
    "    gender_counts.plot(kind=\"bar\", stacked=True)\n",
    "    plt.title(\"ðŸ” Prediction Correctness by Gender\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Column 'Gender' not found for subgroup analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 ANN + ADASYN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ðŸ¤– ANN with ADASYN â€” Full Evaluation + SHAP\n",
    "# =====================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, roc_auc_score, roc_curve, auc,\n",
    "    precision_score, recall_score, log_loss, mean_squared_error, confusion_matrix\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Assumes X and y are already defined as pandas DataFrames/Series\n",
    "X_array = X.to_numpy()\n",
    "y_array = y.to_numpy()\n",
    "\n",
    "f1_scores, acc_scores, auc_scores = [], [], []\n",
    "prec_scores, rec_scores, logloss_scores, mse_scores = [], [], [], []\n",
    "conf_matrices = []\n",
    "\n",
    "n_splits = 5\n",
    "random_seed = 42\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_array, y_array), 1):\n",
    "    print(f\"\\nðŸ” Fold {fold}/{n_splits}\")\n",
    "    X_train, X_test = X_array[train_idx], X_array[test_idx]\n",
    "    y_train, y_test = y_array[train_idx], y_array[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # âœ… Impute missing values before using ADASYN\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_scaled = imputer.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = imputer.transform(X_test_scaled)\n",
    "\n",
    "    adasyn = ADASYN(random_state=random_seed)\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_resampled, y_train_resampled, validation_split=0.2, epochs=50, batch_size=32,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "              verbose=0)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test_scaled).ravel()\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    prec_scores.append(precision_score(y_test, y_pred))\n",
    "    rec_scores.append(recall_score(y_test, y_pred))\n",
    "    auc_val = roc_auc_score(y_test, y_pred_probs)\n",
    "    auc_scores.append(auc_val)\n",
    "    logloss_scores.append(log_loss(y_test, y_pred_probs))\n",
    "    mse_scores.append(mean_squared_error(y_test, y_pred_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    aucs.append(auc_val)\n",
    "    plt.plot(fpr, tpr, alpha=0.3, label=f'Fold {fold} ROC (AUC = {auc_val:.2f})')\n",
    "\n",
    "# ðŸ“ˆ ROC Curve\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f} Â± {std_auc:.2f})', lw=2)\n",
    "plt.title('ðŸ“ˆ ROC Curve - ANN + ADASYN (5-Fold CV)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Š Averaged Confusion Matrix\n",
    "avg_cm = np.mean(conf_matrices, axis=0).astype(int)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(avg_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Dropout', 'Graduate'],\n",
    "            yticklabels=['Dropout', 'Graduate'])\n",
    "plt.title(\"ðŸ” Averaged Confusion Matrix (ANN + ADASYN)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# âœ… SHAP (KernelExplainer â€” CPU compatible)\n",
    "X_train_sample = X_train_resampled[:100]\n",
    "X_test_sample = X_test_scaled[:100]\n",
    "predict_fn = lambda x: model.predict(x).ravel()\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_fn, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "shap.summary_plot(shap_values, features=X_test_sample, feature_names=X.columns)\n",
    "\n",
    "# ðŸ“Š Summary Metrics\n",
    "print(\"\\nðŸ“Š ANN + ADASYN Summary:\")\n",
    "print(f\"âœ… Accuracy:   {np.mean(acc_scores):.4f} Â± {np.std(acc_scores):.4f}\")\n",
    "print(f\"âœ… Precision:  {np.mean(prec_scores):.4f} Â± {np.std(prec_scores):.4f}\")\n",
    "print(f\"âœ… Recall:     {np.mean(rec_scores):.4f} Â± {np.std(rec_scores):.4f}\")\n",
    "print(f\"âœ… F1 Score:   {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}\")\n",
    "print(f\"âœ… ROC-AUC:    {np.mean(auc_scores):.4f} Â± {np.std(auc_scores):.4f}\")\n",
    "print(f\"âœ… MSE:        {np.mean(mse_scores):.4f} Â± {np.std(mse_scores):.4f}\")\n",
    "print(f\"âœ… Log Loss:   {np.mean(logloss_scores):.4f} Â± {np.std(logloss_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
